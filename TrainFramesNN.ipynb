{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file should contain CNN-type neural networks that process sentences as frame+variable\n",
    "Presently, it is implemented with keras\n",
    "\"\"\"\n",
    "import warnings # suppress some warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, Dense, MaxPooling1D, MaxPooling2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay, InverseTimeDecay\n",
    "from tensorflow.keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretend_X = []\n",
    "# pretend_sentences = [\"I have one dream .\", \"The plane landed outside of the airport .\", \"testing testing\"]\n",
    "# pretend_max_sent_len = 10\n",
    "# for s in pretend_sentences:\n",
    "#     vecs = np.zeros((pretend_max_sent_len, 300))\n",
    "#     tokens = nlp(s)\n",
    "#     for i, t in enumerate(tokens):\n",
    "#         vecs[i] = t.vector\n",
    "#     pretend_X.append(vecs)\n",
    "# pretend_X = np.array(pretend_X)\n",
    "\n",
    "# pretend_Y = [3, 5, 0]\n",
    "# pretend_Y = to_categorical(pretend_Y, num_classes=pretend_max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can add attention? can use recurrent structure?\n",
    "class textToVarNN:\n",
    "    # can increase if computer has GPU (may want to check if GPU is in use by tensorflow)\n",
    "    batch_size = 16 \n",
    "    # at least 200 for actual training, use less epochs for tweaking when you can see patterns quickly\n",
    "    epochs = 20\n",
    "    max_sentence_len = 10 # EDITable value; make sure same for both networks\n",
    "    word_vec_len = 300 # shape of spacy word vec\n",
    "\n",
    "    def __init__(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # input indicates shape of X for a single entry\n",
    "        model.add(Input(shape=(self.max_sentence_len, self.word_vec_len)))\n",
    "        \n",
    "        # the middle layers can be TWEAKed\n",
    "        # to find a configuration that gives good results\n",
    "        # candidate layers: Conv1D, Conv2D, Dense, Dropout, Flatten, MaxPooling1D, MaxPooling2D\n",
    "        # candidate activations: relu, softmax, sigmoid, None\n",
    "        # an advanced activation is also available as a layer: LeakyReLU()\n",
    "        # Note: it is not usually useful to have more than 10 layers\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(48))\n",
    "        \n",
    "        # output has to be within sentence len, \n",
    "        # since training Y is going to be one-hot vectors \n",
    "        # indicating which one of the input words is the variable.\n",
    "        # alternatively, look into the CategoryEncoding layer \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.max_sentence_len, activation='softmax'))\n",
    "        \n",
    "        # the learning rate can be TWEAKed\n",
    "        # see https://keras.io/api/optimizers/ \n",
    "        # see https://keras.io/api/optimizers/learning_rate_schedules/\n",
    "        lr_schedule = ExponentialDecay(\n",
    "            initial_learning_rate=1e-2,\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.9)\n",
    "        optimizer = Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=optimizer)\n",
    "        self.model = model\n",
    "\n",
    "        # Note: if it runs very slow, or stalls the computer, \n",
    "        # the reason might be too many trainable parameters.\n",
    "        # remember a batch of X, Y, and the parameters are all\n",
    "        # held in memory when this is being trained\n",
    "        print(self.model.summary())\n",
    "\n",
    "    # takes 2 ndarrays as input\n",
    "    def train(self, X, Y, batch_size=batch_size, epochs=epochs):\n",
    "        # when it trains, the validation error should be trending down\n",
    "        self.model.fit(X, Y, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "    # takes 1 ndarray as input\n",
    "    def predict(self, X):            \n",
    "        # return one from predictions using weighted chance\n",
    "        pred_Y = self.model.predict(X)\n",
    "        # picks a random index, using probabilities weighted by pred_Y[i]\n",
    "        pred = [np.random.choice(range(len(x)), p=x) for x in pred_Y]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = textToVarNN()\n",
    "# test.train(pretend_X, pretend_Y, epochs=4)\n",
    "# testpred = test.predict(pretend_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(test.model, \"temp.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCOMPLETE\n",
    "class frameToVarNN:\n",
    "    # can increase if computer has GPU (may want to check if GPU is in use by tensorflow)\n",
    "    batch_size = 16 \n",
    "    # at least 200 for actual training, use less epochs for tweaking when you can see patterns quickly\n",
    "    epochs = 20\n",
    "    max_sentence_len = 10 # EDITable value; make sure same for both networks\n",
    "    word_vec_len = 300 # shape of spacy word vec\n",
    "    \n",
    "    def __init__(self):\n",
    "        frame_input = Input(shape=(self.max_sentence_len, self.word_vec_len), name='frame')\n",
    "        varindex_input = Input(shape=(self.max_sentence_len, ), name='varindex') # can use embedding layer here\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        # when it trains, the validation error should be trending down\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        # return one from predictions using weighted chance\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
