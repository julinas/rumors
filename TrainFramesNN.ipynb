{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file should contain CNN-type neural networks that process sentences as frame+variable\n",
    "Presently, it is implemented with keras\n",
    "\"\"\"\n",
    "import warnings # suppress some warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Input, Reshape, Embedding\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, Dense, MaxPooling1D, MaxPooling2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay, InverseTimeDecay\n",
    "from tensorflow.keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helper import MongoHelper\n",
    "from tokenizer import Token\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144688\n"
     ]
    }
   ],
   "source": [
    "helper = MongoHelper()\n",
    "\n",
    "X = []\n",
    "entries = helper.getAllEntries()\n",
    "max_sent_len = 30\n",
    "Y=[]\n",
    "count = 0\n",
    "for e in entries:\n",
    "    sentence = e[0]\n",
    "    sentenceSplit = sentence.split()\n",
    "    vecs = np.zeros((max_sent_len, 300))\n",
    "    tokens = nlp(sentence)\n",
    "    if (len(sentenceSplit)<= max_sent_len and len(tokens) <= max_sent_len):\n",
    "        for i, t in enumerate(tokens):\n",
    "            vecs[i] = t.vector\n",
    "                \n",
    "        X= vecs\n",
    "        Y=e[1][\"variablePosition\"]\n",
    "        x = np.array(X)\n",
    "        y = to_categorical(Y, num_classes = max_sent_len)\n",
    "            \n",
    "        x=x.reshape(1,30,300)\n",
    "        y=y.reshape(1,30)\n",
    "        count+=1\n",
    "    else:\n",
    "        pass\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getGenerator():\n",
    "    helper = MongoHelper()\n",
    "\n",
    "    X = []\n",
    "    entries = helper.getAllEntries()\n",
    "    max_sent_len = 30\n",
    "    Y=[]\n",
    "    count = 0\n",
    "    for e in entries:\n",
    "        sentence = e[0]\n",
    "        sentenceSplit = sentence.split()\n",
    "        vecs = np.zeros((max_sent_len, 300))\n",
    "        tokens = nlp(sentence)\n",
    "        if (len(sentenceSplit)<= max_sent_len and len(tokens) <= max_sent_len):\n",
    "            for i, t in enumerate(tokens):\n",
    "                vecs[i] = t.vector\n",
    "                \n",
    "            X= vecs\n",
    "            Y=e[1][\"variablePosition\"]\n",
    "            x = np.array(X)\n",
    "            y = to_categorical(Y, num_classes = max_sent_len)\n",
    "            \n",
    "            x=x.reshape(1,30,300)\n",
    "            y=y.reshape(1,30)\n",
    "            yield (x,y)\n",
    "            \n",
    "\n",
    "    \n",
    "#     yield (X,Y)\n",
    "\n",
    "# pretend_X = []\n",
    "# pretend_sentences = [\"I have one dream .\", \"The plane landed outside of the airport .\", \"testing testing\"]\n",
    "# pretend_max_sent_len = 10\n",
    "# for s in pretend_sentences:\n",
    "#     vecs = np.zeros((pretend_max_sent_len, 300))\n",
    "#     tokens = nlp(s)\n",
    "#     for i, t in enumerate(tokens):\n",
    "#         vecs[i] = t.vector\n",
    "#     pretend_X.append(vecs)\n",
    "# pretend_X = np.array(pretend_X)\n",
    "\n",
    "# pretend_Y = [3, 5, 0]\n",
    "# pretend_Y = to_categorical(pretend_Y, num_classes=pretend_max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can add attention? can use recurrent structure?\n",
    "#train_on_batch (rather than model.fit), fit w/ generator\n",
    "class textToVarNN:\n",
    "    # can increase if computer has GPU (may want to check if GPU is in use by tensorflow)\n",
    "    batch_size = 48\n",
    "    # at least 200 for actual training, use less epochs for tweaking when you can see patterns quickly\n",
    "    epochs =3\n",
    "    max_sentence_len = 30 # EDITable value; make sure same for both networks\n",
    "    word_vec_len = 300 # shape of spacy word vec\n",
    "    steps_per_epoch = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # input indicates shape of X for a single entry\n",
    "        model.add(Input(shape=(self.max_sentence_len, self.word_vec_len)))\n",
    "        \n",
    "        # the middle layers can be TWEAKed\n",
    "        # to find a configuration that gives good results\n",
    "        # candidate layers: Conv1D, Conv2D, Dense, Dropout, Flatten, MaxPooling1D, MaxPooling2D\n",
    "        # candidate activations: relu, softmax, sigmoid, None\n",
    "        # an advanced activation is also available as a layer: LeakyReLU()\n",
    "        # Note: it is not usually useful to have more than 10 layers\n",
    "        # Note: first dimension of a layer is most efficient at 2^n for some n\n",
    "        \n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Conv1D(filters=100, kernel_size=8, padding='same', activation='softmax'))\n",
    "        model.add(MaxPooling1D(pool_size=16))\n",
    "        model.add(Flatten())\n",
    "        MaxPooling2D(pool_size=(30, 30), strides=None, padding=\"valid\", data_format=None)\n",
    "        model.add(Dropout(.2))\n",
    "        \n",
    "        \n",
    "        # output has to be within sentence len, \n",
    "        # since training Y is going to be one-hot vectors \n",
    "        # indicating which one of the input words is the variable.\n",
    "        # alternatively, look into the CategoryEncoding layer \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.max_sentence_len, activation='softmax'))\n",
    "        \n",
    "        # the learning rate can be TWEAKed\n",
    "        # see https://keras.io/api/optimizers/ \n",
    "        # see https://keras.io/api/optimizers/learning_rate_schedules/\n",
    "        lr_schedule = ExponentialDecay(\n",
    "            initial_learning_rate=1e-2,\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.9)\n",
    "        optimizer = Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=optimizer)\n",
    "        self.model = model\n",
    "\n",
    "        # Note: if it runs very slow, or stalls the computer, \n",
    "        # the reason might be too many trainable parameters.\n",
    "        # remember a batch of X, Y, and the parameters are all\n",
    "        # held in memory when this is being trained\n",
    "        print(self.model.summary())\n",
    "    \n",
    "\n",
    "    # takes 2 ndarrays as input\n",
    "    #def train(self, X, Y, batch_size=batch_size, epochs=epochs):\n",
    "    \n",
    "    #def train(self, batch_size=batch_size, epochs=epochs):\n",
    "    def train(self, steps_per_epoch=steps_per_epoch, epochs=epochs):\n",
    "    #def train(self):\n",
    "        # when it trains, the validation error should be trending down\n",
    "        #self.model.fit(X, Y, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "        \n",
    "        generator=getGenerator()\n",
    "        #self.model.fit(generator, batch_size=batch_size, epochs=epochs)\n",
    "        self.model.fit(generator, steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "        #self.model.train_on_batch(entries[0],entries[1])\n",
    "\n",
    "    # takes 1 ndarray as input\n",
    "    def predict(self, X):            \n",
    "        # return one from predictions using weighted chance\n",
    "        pred_Y = self.model.predict(X)\n",
    "        # picks a random index, using probabilities weighted by pred_Y[i]\n",
    "        pred = [np.random.choice(range(len(x)), p=x) for x in pred_Y]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (None, 30, 128)           38528     \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 30, 32)            4128      \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 30, 100)           25700     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 30)                3030      \n",
      "=================================================================\n",
      "Total params: 71,386\n",
      "Trainable params: 71,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 3.2464\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 3.0211\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 3.0505\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 3.1323\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 3.2789\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 3.2867\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 3.5255\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 3.3587\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 3.1746\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 3.1520\n"
     ]
    }
   ],
   "source": [
    "test = textToVarNN()\n",
    "#test.train(X, Y, epochs=4)\n",
    "test.train(steps_per_epoch=100, epochs=10)\n",
    "#test.train()\n",
    "#testpred = test.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(test.model, \"temp.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretend_X_frame = []\n",
    "pretend_X_varindex = np.array([2, 6, 1])\n",
    "pretend_Y_var = []\n",
    "pretend_sentences = [\"I have one dream .\", \"The plane landed outside of the airport .\", \"testing testing\"]\n",
    "pretend_max_sent_len = 10\n",
    "for j, s in enumerate(pretend_sentences):\n",
    "    vecs = np.zeros((pretend_max_sent_len, 300))\n",
    "    tokens = nlp(s)\n",
    "    for i, t in enumerate(tokens):\n",
    "        if pretend_X_varindex[j] == i:\n",
    "            pretend_Y_var.append(t.vector)\n",
    "        else:\n",
    "            vecs[i] = t.vector\n",
    "    pretend_X_frame.append(vecs)\n",
    "pretend_X_frame = np.array(pretend_X_frame) \n",
    "pretend_Y_var = np.array(pretend_Y_var)\n",
    "\n",
    "pretend_X_varindex = to_categorical(pretend_X_varindex, num_classes=pretend_max_sent_len)\n",
    "\n",
    "pretend_Y_frame = pretend_X_frame\n",
    "pretend_Y_varindex = pretend_X_varindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use GloVe embedding directly from keras, rather than through SpaCy. \n",
    "class frameToVarNN:\n",
    "    # can increase if computer has GPU (may want to check if GPU is in use by tensorflow)\n",
    "    batch_size = 16 \n",
    "    # at least 200 for actual training, use less epochs for tweaking when you can see patterns quickly\n",
    "    epochs = 20\n",
    "    max_sentence_len = 10 # EDITable value; make sure same for both networks\n",
    "    word_vec_len = 300 # shape of spacy word vec\n",
    "    \n",
    "    def __init__(self):\n",
    "        # here, it probably makes the job for the nn easier if we leave a blank at varindex for frame input\n",
    "        frame_input = Input(shape=(self.max_sentence_len, self.word_vec_len), name='frame_input')\n",
    "        varindex_input = Input(shape=(self.max_sentence_len, ), name='varindex_input') # can use embedding layer here\n",
    "        \n",
    "        # the middle layers can be TWEAKed\n",
    "        \n",
    "        frame_features = Dense(128)(frame_input)\n",
    "        frame_features = Flatten()(frame_features)\n",
    "        \n",
    "        varindex_features = varindex_input\n",
    "        \n",
    "        features = concatenate([frame_features, varindex_features])\n",
    "        features = Dense(128)(features)\n",
    "        features = Dense(self.max_sentence_len * 16)(features)\n",
    "        features = Reshape((self.max_sentence_len, 16))(features)\n",
    "\n",
    "        frame_pred = Dense(self.word_vec_len, name='frame_pred')(features)\n",
    "        \n",
    "        features = Flatten()(features)\n",
    "        varindex_pred = Dense(self.max_sentence_len, name='varindex_pred', activation='softmax')(features)\n",
    "        var_pred = Dense(self.word_vec_len, name='var_pred')(features)\n",
    "        \n",
    "        model = Model(inputs=[frame_input, varindex_input], outputs=[frame_pred, varindex_pred, var_pred])\n",
    "        \n",
    "        # the learning rate can be TWEAKed\n",
    "        # see https://keras.io/api/optimizers/ \n",
    "        # see https://keras.io/api/optimizers/learning_rate_schedules/\n",
    "        lr_schedule = ExponentialDecay(\n",
    "            initial_learning_rate=1e-2,\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.9)\n",
    "        optimizer = Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        model.compile(loss=[\n",
    "            MeanSquaredError(),\n",
    "            CategoricalCrossentropy(),\n",
    "            MeanSquaredError()\n",
    "        ], optimizer=optimizer)\n",
    "\n",
    "        self.model = model\n",
    "#         print(model.summary())\n",
    "        \n",
    "    def train(self, X_frame, X_varindex, Y_frame, Y_varindex, Y_var, batch_size=batch_size, epochs=epochs):\n",
    "        # when it trains, the validation error should be trending down\n",
    "        self.model.fit(\n",
    "            {'frame_input': X_frame, 'varindex_input': X_varindex},\n",
    "            {'frame_pred': Y_frame, 'varindex_pred': Y_varindex, 'var_pred': Y_var},\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "    \n",
    "    def predict(self, X_frame, X_varindex):\n",
    "        # TODO: return one variable from predictions using weighted chance\n",
    "        # TODO requires predicting categorical var rather than word vector\n",
    "        _, pred_index, pred_var = self.model.predict([X_frame, X_varindex])\n",
    "        # picks a random index, using probabilities weighted by pred_index[i]\n",
    "        pred_index = [np.random.choice(range(len(x)), p=x) for x in pred_index]\n",
    "        return (pred_index, pred_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = frameToVarNN()\n",
    "# test.train(pretend_X_frame[:2], pretend_X_varindex[:2], pretend_Y_frame[:2], pretend_Y_varindex[:2], pretend_Y_var[:2],\n",
    "#            epochs=6)\n",
    "# testpred = test.predict(pretend_X_frame, pretend_X_varindex)\n",
    "# print(testpred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(test.model, \"temp.png\", show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
